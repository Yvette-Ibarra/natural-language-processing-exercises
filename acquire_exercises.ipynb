{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afefa6db",
   "metadata": {},
   "source": [
    "# Acquire Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe80654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26daf43a",
   "metadata": {},
   "source": [
    "# 1. Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}\n",
    "```\n",
    "Plus any additional properties you think might be helpful.\n",
    "\n",
    "Bonus: Scrape the text of all the articles linked on codeup's blog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57582662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url\n",
    "url='https://codeup.com/blog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup\n",
    "soup = BeautifulSoup(requests.get(url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see soup\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request text\n",
    "html_text =requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b628849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3821bcd",
   "metadata": {},
   "source": [
    "I got code 403; trying to figure a way to acess the information.\n",
    "\n",
    "New attempt follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define headers\n",
    "headers = {'User-Agent': 'Codeup Data Science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a15724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a request\n",
    "response = requests.get('https://codeup.com/data-science/become-a-data-scientist/', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the soup and investigate\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5494d",
   "metadata": {},
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a15938",
   "metadata": {},
   "outputs": [],
   "source": [
    "title= soup.find('h1')\n",
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the date published\n",
    "publish_date = soup.find('span', class_='published')\n",
    "publish_date.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fbb27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Access the article content\n",
    "content = soup.find('div', class_='entry-content')\n",
    "content.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = soup.find_all('a', 'href')\n",
    "article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"a\" tag\n",
    "element = soup.find('a')\n",
    "# Get the attribute value\n",
    "data = element.get('href')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1f51a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the \"a\" tag\n",
    "element = soup.find_all('a')\n",
    "# Get the attribute value\n",
    "element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fa1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_url_list= ['https://codeup.com/data-science/become-a-data-scientist/',\n",
    "                'https://codeup.com/employers/hiring-tech-talent/',\n",
    "               'https://codeup.com/cloud-administration/cap-funding-options/'\n",
    "               'https://codeup.com/dallas-info/it-professionals-dallas/',\n",
    "               'https://codeup.com/codeup-news/codeup-voted-1-technical-school-in-dfw/',\n",
    "               'https://codeup.com/tips-for-prospective-students/financing/codeups-scholarships/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(url):\n",
    "    blog_info={\"title\",\"publish_date\",\"content\"}\n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    #find the title\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1')\n",
    "    \n",
    "    \n",
    "    #Access the date published\n",
    "    publish_date = soup.find('span', class_='published')\n",
    " \n",
    "    \n",
    "    #Access the article content\n",
    "    content = soup.find('div', class_='entry-content')\n",
    "\n",
    "    output = {\n",
    "        'title': title.text,\n",
    "        'publish_date': publish_date.text,\n",
    "        'content': content.text\n",
    "     }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845219d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blog_info= []\n",
    "for i in blog_url_list:\n",
    "    output = basic_info(i)\n",
    "    blog_info.append(output)\n",
    "blog_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc932ee",
   "metadata": {},
   "source": [
    "### alternative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "030169b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/blog/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = requests.get(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7601c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish our basic soup with the base url\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94c5e1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"more-link\" href=\"https://codeup.com/data-science/become-a-data-scientist/\">read more</a>,\n",
       " <a class=\"more-link\" href=\"https://codeup.com/employers/hiring-tech-talent/\">read more</a>,\n",
       " <a class=\"more-link\" href=\"https://codeup.com/cloud-administration/cap-funding-options/\">read more</a>,\n",
       " <a class=\"more-link\" href=\"https://codeup.com/dallas-info/it-professionals-dallas/\">read more</a>,\n",
       " <a class=\"more-link\" href=\"https://codeup.com/codeup-news/codeup-voted-1-technical-school-in-dfw/\">read more</a>,\n",
       " <a class=\"more-link\" href=\"https://codeup.com/tips-for-prospective-students/financing/codeups-scholarships/\">read more</a>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create soup)\n",
    "soup.select('.more-link') #soup.find_all('a', class_='more-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cfa09ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"more-link\" href=\"https://codeup.com/data-science/become-a-data-scientist/\">read more</a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('.more-link')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b42f32c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://codeup.com/data-science/become-a-data-scientist/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('.more-link')[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54cf442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://codeup.com/data-science/become-a-data-scientist/',\n",
       " 'https://codeup.com/employers/hiring-tech-talent/',\n",
       " 'https://codeup.com/cloud-administration/cap-funding-options/',\n",
       " 'https://codeup.com/dallas-info/it-professionals-dallas/',\n",
       " 'https://codeup.com/codeup-news/codeup-voted-1-technical-school-in-dfw/',\n",
       " 'https://codeup.com/tips-for-prospective-students/financing/codeups-scholarships/']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [link['href'] for link in soup.select('.more-link')]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59ae8b",
   "metadata": {},
   "source": [
    "process for one link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f384ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = links[0]\n",
    "response = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8962d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Become a Data Scientist in 6 Months!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h1', class_ ='entry-title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bd23cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are you feeling unfulfilled in your work but want to avoid returning to the traditional educational route? Codeup can help! Starting over as a professional is daunting and not always ideal. Codeup can help you go from a career you are bored with, to a job that excites you in just 6 months!\\nHere’s how…\\nData Science Program\\nDuring our 20-week program, you will have the opportunity to take your career to new heights with data science being one of the most needed jobs in tech.\\nYou’ll gather data, then clean it, explore it for trends, and apply machine learning models to make predictions.\\nUpon completing this program, you will know how to turn insights into actionable recommendations. You’ll be a huge asset to any company, having all the technical skills to become a data scientist with projects upon projects of experience under your belt.\\nCodeup\\nA common reason individuals opt not to change their careers is fear it is too late. Codeup has crafted a program that will guide you through your career transition and prove that you can jumpstart a new job at any age and experience level.\\nWhen you decide to attend Codeup, you get a support system. First, our admissions team will guide you through getting started as well as assist with getting your financial aid squared away.\\nOnce your program begins, you will have industry experts as your instructors who will help you reach your full potential and complete the program successfully.\\nNext, you will have a student experience team ready to assist you with navigating through the program and any of life’s obstacles that may arise.\\nFinally, you have a placement team to help you secure the job you’ll work so hard to land!\\n\\nIf you do not land a job within six months of completing a Codeup program, we will refund 100% of your paid tuition.\\nIf you aren’t sure if a career in data science is right for you, consider attending our 100% free instructor-led workshops. These workshops are designed to give you a taste of what you’ll learn in our programs, as well as give you information on financial aid and admissions.\\nWe can also provide program-specific information to your inbox at any time! Get started now or give us a call at (210) 802-7289 and we’ll walk you through the process.\\nJumpstart your tech career with the first and only accredited bootcamp in Texas!*\\n*Codeup is accredited for postsecondary, non-degree granting, career and technology programs by the Middle States Commission on Secondary Schools of the Middle States Association of Colleges and Schools, 3624 Market Street, Philadelphia, PA 19104, (267) 284-5000'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div',class_ = 'entry-content').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445ef7b",
   "metadata": {},
   "source": [
    "put it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1c6bcc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://codeup.com/blog/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeup Data Science\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget\u001b[49m(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m links \u001b[38;5;241m=\u001b[39m [link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.more-link\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = 'https://codeup.com/blog/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "links = [link['href'] for link in soup.select('.more-link')]\n",
    "\n",
    "articles = []\n",
    "\n",
    "for url in links:\n",
    "    \n",
    "    url_response = reqget(url, headers=headers)\n",
    "    soup = BeautifulSoup(url_response.text)\n",
    "    \n",
    "    title = soup.find('h1', class_='entry-title').text\n",
    "    content = soup.find('div', class_='entry-content').text.strip()\n",
    "    \n",
    "    article_dict = {\n",
    "        'title': title,\n",
    "        'content': content\n",
    "    }\n",
    "    \n",
    "    articles.append(article_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62705782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1569e80e",
   "metadata": {},
   "source": [
    "# 2. News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "* Business\n",
    "* Sports\n",
    "* Technology\n",
    "* Entertainment\n",
    "\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:\n",
    "``` python\n",
    "\n",
    "{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}\n",
    "```\n",
    "Hints:\n",
    "\n",
    "a. Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "\n",
    "b. Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "\n",
    "c. Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "\n",
    "d. Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any additional processing that needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url\n",
    "url = 'https://inshorts.com/en/read/technology'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content\n",
    "response = requests.get(url)\n",
    "response = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86ff06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to get html code, nicer looking\n",
    "soup = BeautifulSoup(response,'html.parser' )\n",
    "#soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get main branch\n",
    "card_stack = soup.find('div',class_ ='card-stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get articles\n",
    "article_info = card_stack.find('div', class_ ='news-card z-depth-1')\n",
    "article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef0ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get content\n",
    "content = article_info.find('div', itemprop='articleBody').text\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414d9fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get title\n",
    "a_ = article_info.find('a')\n",
    "#headline = a_.find_all('span', attr)\n",
    "title = a_.find(itemprop ='headline').get_text()\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb29cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get published date\n",
    "date = article_info.find('span', class_= 'date').text\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603186cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author\n",
    "author = article_info.find('span', class_= 'author').text\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37079d5c",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "<div class=\"news-card z-depth-1\" itemscope=\"\" itemtype=\"http://schema.org/NewsArticle\">\n",
    "  <span content=\"\" itemscope=\"\" itemprop=\"mainEntityOfPage\" itemtype=\"https://schema.org/WebPage\" itemid=\"https://inshorts.com/en/news/billie-eilishs-home-address-revealed-to-178-lakh-people-by-crimereporting-app-1673091145284\"></span>\n",
    "  <span itemtype=\"https://schema.org/Person\" itemscope=\"itemscope\" itemprop=\"author\">\n",
    "    <span itemprop=\"name\" content=\"Hiral Goyal\"></span>\n",
    "  </span>\n",
    "  <span itemprop=\"description\" content=\"Billie Eilish's home address revealed to 1.78 lakh people by crime-reporting app\"></span>\n",
    "  <span itemprop=\"image\" itemscope=\"\" itemtype=\"https://schema.org/ImageObject\">\n",
    "    <meta itemprop=\"url\" content=\"https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/01_jan/7_sat/img_1673090366771_702.jpg?\">\n",
    "    <meta itemprop=\"width\" content=\"864\">\n",
    "    <meta itemprop=\"height\" content=\"483\">\n",
    "  </span>\n",
    "  <span itemtype=\"https://schema.org/Organization\" itemscope=\"itemscope\" itemprop=\"publisher\">\n",
    "    <span itemprop=\"url\" content=\"https://inshorts.com/\"></span>\n",
    "    <span itemprop=\"name\" content=\"Inshorts\"></span>\n",
    "    <span itemprop=\"logo\" itemscope=\"\" itemtype=\"https://schema.org/ImageObject\">\n",
    "      <span itemprop=\"url\" content=\"https://assets.inshorts.com/inshorts/images/v1/variants/jpg/m/2018/11_nov/21_wed/img_1542823931298_497.jpg\"></span>\n",
    "      <meta itemprop=\"width\" content=\"400\">\n",
    "      <meta itemprop=\"height\" content=\"60\">\n",
    "    </span>\n",
    "  </span>\n",
    "  <div class=\"news-card-image\" style=\"background-image: url('https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2023/01_jan/7_sat/img_1673090366771_702.jpg?')\">\n",
    "  </div>\n",
    "  <div class=\"news-card-title news-right-box\">\n",
    "    <a class=\"clickable\" onclick=\"track_GA_Mixpanel({'hitType': 'event', 'category': 'TitleOfNews', 'action': 'clicked', 'label': 'Billie%20Eilish's%20home%20address%20revealed%20to%201.78%20lakh%20people%20by%20crime-reporting%20app)' });\" style=\"color:#44444d!important\" href=\"/en/news/billie-eilishs-home-address-revealed-to-178-lakh-people-by-crimereporting-app-1673091145284\">\n",
    "      <span itemprop=\"headline\">Billie Eilish's home address revealed to 1.78 lakh people by crime-reporting app</span>\n",
    "    </a>\n",
    "    <div class=\"news-card-author-time news-card-author-time-in-title\">\n",
    "      <a href=\"/prev/en/news/billie-eilishs-home-address-revealed-to-178-lakh-people-by-crimereporting-app-1673091145284\"><span class=\"short\">short</span></a> by <span class=\"author\">Hiral Goyal</span> / \n",
    "      <span class=\"time\" itemprop=\"datePublished\" content=\"2023-01-07T11:32:25.000Z\">05:02 pm</span> on <span clas=\"date\">07 Jan 2023,Saturday</span>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"news-card-content news-right-box\">\n",
    "    <div itemprop=\"articleBody\">Singer Billie Eilish's family home location was leaked after crime-reporting app Citizen published her address to thousands of people following an alleged burglary at her home. After notifying its users about a break-in in a Los Angeles neighbourhood, Citizen updated the message to state the house belonged to Eilish. The alert was sent to 1,78,000 people and viewed by 78,000.</div>\n",
    "    <div class=\"news-card-author-time news-card-author-time-in-content\">\n",
    "      <a href=\"/prev/en/news/billie-eilishs-home-address-revealed-to-178-lakh-people-by-crimereporting-app-1673091145284\"><span class=\"short\">short</span></a> by <span class=\"author\">Hiral Goyal</span> / \n",
    "      <span class=\"time\" itemprop=\"dateModified\" content=\"2023-01-07T11:32:25.000Z\">05:02 pm</span> on <span class=\"date\">07 Jan</span>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "    <div class=\"news-card-footer news-right-box\">\n",
    "      <div class=\"read-more\">read more at <a class=\"source\" onclick=\"track_GA_Mixpanel({'hitType': 'event', 'category': 'ReadMore', 'action': 'clicked', 'label': 'The%20Siasat%20Daily' });\" target=\"_blank\" href=\"https://www.siasat.com/citizen-app-reveals-exact-location-of-billie-eilishs-family-home-to-178k-users-2497487/?utm_campaign=fullarticle&amp;utm_medium=referral&amp;utm_source=inshorts \">The Siasat Daily</a></div>\n",
    "    </div>\n",
    "  \n",
    "\n",
    "</div>\n",
    "\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51aaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source link\n",
    "output = article_info.find('a', class_= 'source')\n",
    "pattern = re.compile(r'(www.*?)\\s')\n",
    "link = pattern.findall(str(output))\n",
    "link[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    output =[]\n",
    "    response = requests.get(url)\n",
    "    response = response.content \n",
    "    soup = BeautifulSoup(response,'html.parser')\n",
    "    \n",
    "    # main branch\n",
    "    card_stack = soup.find('div',class_ ='card-stack')\n",
    "    # article branch\n",
    "    article_info = card_stack.find('div', class_ ='news-card z-depth-1')\n",
    "     \n",
    "    # get title\n",
    "    a_ = article_info.find('a')\n",
    "    #headline = a_.find_all('span', attr)\n",
    "    title = a_.find(itemprop ='headline').get_text()\n",
    "    \n",
    "    # get published date\n",
    "    date = article_info.find('span', class_= 'date').text\n",
    "    \n",
    "    # get author\n",
    "    author = article_info.find('span', class_= 'author').text\n",
    "    \n",
    "    # get content\n",
    "    content = article_info.find('div', itemprop='articleBody').text\n",
    "    \n",
    "    # get source link\n",
    "    source = article_info.find('a', class_= 'source')\n",
    "    pattern = re.compile(r'(www.*?)\\s')\n",
    "    link = pattern.findall(str(source))\n",
    "    link = link[0]\n",
    "    \n",
    "    output.append([title,date,author,content,link])\n",
    "    \n",
    "    output_df = pd.DataFrame(output, columns = ['title','date','author','content','source_link'])\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea9a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbec62",
   "metadata": {},
   "source": [
    "The above method only works for one article on a page and does not seem to be able to work on next article. Possible reason when first acquired article only one came along. A diffirent method must me used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbc1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aefe798",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.inshorts.com/en/read/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d02f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify class to select all the news cards\n",
    "cards = soup.select('.news-card')\n",
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc38730",
   "metadata": {},
   "outputs": [],
   "source": [
    "card = cards[0]\n",
    "#card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline\n",
    "headline = card.find('span', itemprop = 'headline').text\n",
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author\n",
    "card.find('span', class_ = 'author').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38929ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content\n",
    "card.find('div', itemprop = 'articleBody').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3676c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date\n",
    "card.find('span', clas ='date').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1321e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source link\n",
    "# get source link\n",
    "source = card.find('a', class_= 'source')\n",
    "pattern = re.compile(r'(www.*?)\\s')\n",
    "link = pattern.findall(str(source))\n",
    "link[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde37da0",
   "metadata": {},
   "source": [
    "### alternative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90355b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.inshorts.com/en/read/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bf04e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Drunk man smoked in toilet, another peed on woman's blanket on Air India flight\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get title\n",
    "soup.find_all('span', itemprop = 'headline')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa66dbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A drunk passenger smoked in the toilet on a Paris-Delhi Air India flight on December 6, the DGCA said. This is the same Paris-Delhi flight on which another drunk man urinated on a woman co-passenger's blanket when she went to the lavatory. Separately, Shankar Mishra was arrested for urinating on a woman on Air India's November 26 New York-Delhi flight.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get content\n",
    "soup.find_all('div', itemprop = 'articleBody')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a6b7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['national',\n",
       " 'business',\n",
       " 'sports',\n",
       " 'world',\n",
       " 'politics',\n",
       " 'technology',\n",
       " 'startup',\n",
       " 'entertainment',\n",
       " 'miscellaneous',\n",
       " 'hatke',\n",
       " 'science',\n",
       " 'automobile']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get categories\n",
    "categories = [li.text.lower() for li in soup.select('li')[1:]]\n",
    "categories[0]='national'\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d0d5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it together\n",
    "categories = [li.text.lower() for li in soup.select('li')[1:]]\n",
    "categories[0]='national'\n",
    "\n",
    "inshorts = []\n",
    "for cat in categories:\n",
    "    url = 'https://www.inshorts.com/en/read' + '/'+ cat\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    titles = [span.text for span in soup.find_all('span', itemprop = 'headline')]\n",
    "    contents = [div.text for div in soup.find_all('div', itemprop = 'articleBody')]\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        \n",
    "        article = {\n",
    "            \n",
    "            'title': titles[i],\n",
    "            'content': contents [i],\n",
    "            'category': cat\n",
    "        }\n",
    "\n",
    "        inshorts.append(article)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff069d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': \"Drunk man smoked in toilet, another peed on woman's blanket on Air India flight\",\n",
       "  'content': \"A drunk passenger smoked in the toilet on a Paris-Delhi Air India flight on December 6, the DGCA said. This is the same Paris-Delhi flight on which another drunk man urinated on a woman co-passenger's blanket when she went to the lavatory. Separately, Shankar Mishra was arrested for urinating on a woman on Air India's November 26 New York-Delhi flight.\",\n",
       "  'category': 'national'},\n",
       " {'title': 'Coaching centre run by Rajasthan paper leak case accused demolished in Jaipur',\n",
       "  'content': 'The Jaipur Development Authority (JDA) on Monday demolished a five-storey building of a coaching centre run by Suresh Dhaka, whose name appeared in the second-grade teacher recruitment examination paper leak case. The JDA found that the Adhigam Coaching Centre building was built in violation of laws. The coaching institute was served the notice twice, an official said.',\n",
       "  'category': 'national'},\n",
       " {'title': 'Joshimath divided into 3 zones, govt says most damaged buildings to be demolished',\n",
       "  'content': \"Uttarakhand's Joshimath, where a majority of buildings developed cracks, has been divided into three zones based on the magnitude of possible danger. RM Sundaram, Secretary to CM Pushkar Singh Dhami, said the town has been divided into 'danger', 'buffer' and 'completely safe' zones. Sundaram added that the buildings which have sustained the most damage will be demolished. \",\n",
       "  'category': 'national'},\n",
       " {'title': 'Temporary ban on BS-III petrol & BS-IV diesel cars in Delhi over severe AQI',\n",
       "  'content': 'The Delhi government has decided to impose a temporary ban on plying of BS-III petrol and BS-IV diesel four-wheelers in the national capital from Tuesday in view of severe air quality. \"The ban is likely to be in place till Friday. If the air quality improves, the ban could be lifted before Friday,\" a senior Transport Department official said.',\n",
       "  'category': 'national'},\n",
       " {'title': 'Over 200 shanties gutted in major fire at slum area in Gurugram',\n",
       "  'content': 'More than 200 shanties were destroyed in a massive fire that broke out in a slum near Ghasola village in the Sector 49 area of Gurugram on Monday. No injury was reported in the incident. Over 20 fire engines were pressed into service with 100 firefighters and it took more than four hours to douse the blaze.',\n",
       "  'category': 'national'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inshorts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9bd7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c22909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17af211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d82ec36",
   "metadata": {},
   "source": [
    "# 3. Bonus: cache the data\n",
    "\n",
    "Write your code such that the acquired data is saved locally in some form or fashion. Your functions that retrieve the data should prefer to read the local data instead of having to make all the requests everytime the function is called. Include a boolean flag in the functions to allow the data to be acquired \"fresh\" from the actual sources (re-writing your local cache)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516af7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c30e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d10c90e",
   "metadata": {},
   "source": [
    "# 4. Extra Practice: books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url\n",
    "url ='http://books.toscrape.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response of 200 means it was sucessful\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9de5c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "response_content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a1cdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove hashtag below to see content\n",
    "#response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b175e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to get html code, nicer looking\n",
    "soup = BeautifulSoup(response_content,'html.parser' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c44293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtag below to see soup (html content)\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13da976",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = soup.find('ol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ol.find_all('article', class_='product_pod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71462e0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove hastab below to see list of articles\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the title of the articles\n",
    "for article in articles:\n",
    "    image = article.find('img')\n",
    "    title = image.attrs['alt']\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b429fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the star of the articles\n",
    "for article in articles:\n",
    "    star = article.find('p')\n",
    "    rating = star['class'][1]\n",
    "    print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd202772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the price of the articles\n",
    "for article in articles:\n",
    "    price = article.find('p', class_=\"price_color\").text\n",
    "    # to convert text to float without money symbol\n",
    "    price = float(price[1:])\n",
    "    print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information of one page in book website\n",
    "books =[]\n",
    "for article in articles:\n",
    "    image = article.find('img')\n",
    "    title = image.attrs['alt']\n",
    "    star = article.find('p')\n",
    "    rating = star['class'][1]\n",
    "    price = article.find('p', class_=\"price_color\").text\n",
    "    # to convert text to float without money symbol\n",
    "    price = float(price[1:])\n",
    "    books.append([title,price,rating])\n",
    "books_df = pd.DataFrame(books, columns = ['title','price','stars'])\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get book information of all the pages in website\n",
    "books=[]\n",
    "for i in range(1,51):\n",
    "    \n",
    "    url = f\"http://books.toscrape.com/catalogue/page-{i}.html\"\n",
    "    response = requests.get(url)\n",
    "    response = response.content\n",
    "    soup = BeautifulSoup(response,'html.parser')\n",
    "    ol = soup.find('ol')\n",
    "    artticles = ol.find_all('article', class_ = 'product_pod')\n",
    "\n",
    "\n",
    "\n",
    "    for article in articles:\n",
    "        image = article.find('img')\n",
    "        title = image.attrs['alt']\n",
    "        star = article.find('p')\n",
    "        rating = star['class'][1]\n",
    "        price = article.find('p', class_=\"price_color\").text\n",
    "        # to convert text to float without money symbol\n",
    "        price = float(price[1:])\n",
    "        books.append([title,price,rating])\n",
    "books_df = pd.DataFrame(books, columns = ['title','price','stars'])\n",
    "books_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
