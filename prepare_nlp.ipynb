{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9726c8",
   "metadata": {},
   "source": [
    "# Prepare Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72702313",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a50506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787a688",
   "metadata": {},
   "source": [
    "# 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05bab93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(input_string):\n",
    "    '''\n",
    "    basice_clean function takes in a string and performs the following cleaning: lowercase, normalize characters\n",
    "    and replaces anything that ia a letter , number, whitespace or sigle quote\n",
    "    returns clean_string\n",
    "    '''\n",
    "    # takes original string and lowercase the string\n",
    "    clean_string = input_string.lower()\n",
    "    \n",
    "    # normalized the string\n",
    "    clean_string = unicodedata.normalize('NFKD', clean_string).encode('ascii','ignore').decode('utf-8')\n",
    "    \n",
    "    # remove anything that is not a through z, a number, a single quote, or whites\n",
    "    clean_string = re.sub(r\"[^a-z0-9'\\s]\", '', clean_string)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b252b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = '1-034-@#$.32ksk|llkm fsadpfo-3-ljf &*^...hi mom...?/\\|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d0f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP! @AmazonHelp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "575c6e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103432kskllkm fsadpfo3ljf hi mom'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1256fbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey amazon  my package never arrived httpswwwamazoncomgpcssorderhistoryrefnavordersfirst please fix asap amazonhelp'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99433825",
   "metadata": {},
   "source": [
    "# 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "186bcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_string):\n",
    "    '''\n",
    "    tokenize takes in a string and passes throug basic_clean function then tokenize all the words in the string\n",
    "    returns token_string\n",
    "    '''\n",
    "    \n",
    "    #basic clean string\n",
    "    clean_string = basic_clean(input_string)\n",
    "    \n",
    "    # create tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "     \n",
    "    # apply token to string    \n",
    "    token_string  = tokenizer.tokenize(clean_string, return_str=True)\n",
    "    \n",
    "    return token_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b1a331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103432kskllkm fsadpfo3ljf hi mom'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fc161cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey amazon my package never arrived httpswwwamazoncomgpcssorderhistoryrefnavordersfirst please fix asap amazonhelp'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36446084",
   "metadata": {},
   "source": [
    "# 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23dbd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(input_string):\n",
    "    '''\n",
    "    stem takes in a string and runs it thourgh tokenize function\n",
    "    returns stem_string a stem version of string\n",
    "    '''\n",
    "    # clean string and tokenize\n",
    "    token_string = tokenize(input_string)\n",
    "    \n",
    "    # create stemming object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # stemming string\n",
    "    stem_string = [ps.stem(word) for word in token_string.split()]\n",
    "    # join stemmed string\n",
    "    stem_string = ' '.join(stem_string)\n",
    "    \n",
    "    return stem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b481d63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103432kskllkm fsadpfo3ljf hi mom'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "535e07f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey amazon my packag never arriv httpswwwamazoncomgpcssorderhistoryrefnavordersfirst pleas fix asap amazonhelp'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e5197",
   "metadata": {},
   "source": [
    "# 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06727017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(input_string):\n",
    "    '''\n",
    "    lemmatize takes in a string and passes it through takenize function\n",
    "    returns lemmas_string a lemmatize version of the string.\n",
    "    '''\n",
    "    \n",
    "    # clean string and tokenize\n",
    "    token_string = tokenize(input_string)\n",
    "    \n",
    "    # create object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # apply lemmatizer to string\n",
    "    lemmas_string = [wnl.lemmatize(word) for word in token_string.split()]\n",
    "    lemmas_string = \" \".join(lemmas_string)\n",
    "    \n",
    "    return lemmas_string\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c40854b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103432kskllkm fsadpfo3ljf hi mom'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "295de799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey amazon my package never arrived httpswwwamazoncomgpcssorderhistoryrefnavordersfirst please fix asap amazonhelp'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1c434",
   "metadata": {},
   "source": [
    "# 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f29612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_string, extra_words = None, exclude_words = None):\n",
    "    # ceate stopwords list\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    \n",
    "    # words to be added\n",
    "    stopwords_eng.append(extra_words)\n",
    "    \n",
    "    # words to be removed\n",
    "    stopwords_eng.remove(exclude_words)\n",
    "    \n",
    "    \n",
    "    # this is the stopwords applied(taken out of) the original text\n",
    "    new_string = [word for word in input_string.split() if word not in stopwords_eng]\n",
    "    \n",
    "    return new_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b54fcbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP! @AmazonHelp'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16de05f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1501355399.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [62]\u001b[0;36m\u001b[0m\n\u001b[0;31m    remove_stopwords(sample_text,extra_words=None,'my')\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords(sample_text,extra_words=None,'my')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3f4ea",
   "metadata": {},
   "source": [
    "# 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947e8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148dc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c9968f",
   "metadata": {},
   "source": [
    "# 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06210ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c727d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3bf3b51",
   "metadata": {},
   "source": [
    "# 8. For each dataframe, produce the following columns:\n",
    "\n",
    "* title to hold the title\n",
    "* original to hold the original article/post content\n",
    "* clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "* stemmed to hold the stemmed version of the cleaned data.\n",
    "* lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c01d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46a34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f882aa9",
   "metadata": {},
   "source": [
    "# 9 Ask yourself:\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54ee20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ae52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
